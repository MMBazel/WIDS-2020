{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(style='white')\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Need to go find this one: from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_types(df, num_list, binary_list, string_list):\n",
    "    # Ensure datatypes are correct\n",
    "    for i in df.columns:\n",
    "        if i in num_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in binary_list:\n",
    "            df[i] = df[i].astype('Int64')\n",
    "        elif i in string_list:\n",
    "            df[i] = df[i].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_types_V2(df, num_list, binary_list, string_list):\n",
    "    for i in df.columns:\n",
    "        if i in num_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in binary_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in string_list:\n",
    "            df[i] = df[i].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the necessary files\n",
    "\n",
    "data_dictionary = pd.read_csv('WiDS Datathon 2020 Dictionary.csv')\n",
    "data = pd.read_csv('training_v2.csv')\n",
    "predict_data = pd.read_csv('unlabeled.csv')\n",
    "sample_submission = pd.read_csv('samplesubmission.csv')\n",
    "submission_template = pd.read_csv('solution_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies for safe keeping\n",
    "data_copy = data.copy()\n",
    "predict_data_copy = predict_data.copy()\n",
    "data_dictionary_copy = data_dictionary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns for easier selection in Data Dictionary\n",
    "data_dictionary.columns = ['Category','VariableName','UnitofMeasure','DataType','Descrption','Example']\n",
    "\n",
    "\n",
    "# Preoperly relabel the meta data; Meta data list will be used to pull features and process by type\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'encounter_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'hospital_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'patient_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'hospital_death', 'DataType'] = 'Target'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'bmi', 'DataType'] = 'numeric'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'icu_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'apache_2_diagnosis', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'apache_3j_diagnosis', 'DataType'] = 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train & test data for cleaning & feature engineering\n",
    "\n",
    "all_data = pd.concat([data, predict_data])\n",
    "all_data_copy = all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names by type\n",
    "\n",
    "num_feats_list = []\n",
    "binary_feats_list = []\n",
    "string_feats_list =[]\n",
    "\n",
    "variable_names = list(set(data_dictionary.VariableName))\n",
    "data_dictionary = data_dictionary.set_index('VariableName')\n",
    "\n",
    "\n",
    "for i in variable_names:\n",
    "    #print(data_dictionary.loc[i, 'DataType'])\n",
    "    if (i == 'VariableName') | (i == 'pred') | (i == 'icu_admit_type'):\n",
    "        pass\n",
    "    else :\n",
    "        if data_dictionary.loc[i, 'DataType'] == 'string':\n",
    "            string_feats_list.append(i)    \n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'binary':\n",
    "            binary_feats_list.append(i)\n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'integer':\n",
    "             num_feats_list.append(i)\n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'numeric':\n",
    "             num_feats_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_feats_list.remove('hospital_id')\n",
    "string_feats_list.remove('encounter_id')\n",
    "string_feats_list.remove('patient_id')\n",
    "string_feats_list.remove('icu_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [data, predict_data]\n",
    "for i in dfs:\n",
    "    i = ensure_data_types_V2(i,num_feats_list, binary_feats_list, string_feats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = all_data[all_data.encounter_id.isin(data.encounter_id)]\n",
    "predict_eng = all_data[all_data.encounter_id.isin(predict_data.encounter_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split back to Train & Predict Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikiko.bazeley\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X = train_eng.drop(columns = ['hospital_death'])\n",
    "y = train_eng[['hospital_death']]\n",
    "\n",
    "y['hospital_death'] = y['hospital_death'].astype('Float64')\n",
    "\n",
    "X.drop(columns=['hospital_id','encounter_id','patient_id','icu_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines (Data Cleaning, Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: \n",
      "<class 'category_encoders.target_encoder.TargetEncoder'>\n",
      "Classifier: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "<=================================================================================================>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'knn__n_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d86decb5efb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<=================================================================================================>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mgrid_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mk_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"knn__n_neighbors\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"k_best: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'knn__n_neighbors'"
     ]
    }
   ],
   "source": [
    "search_space = [{\"clf__n_neighbors\": [1,2,5,10,15,200,500]}]\n",
    "\n",
    "winning_encoder = [ce.target_encoder.TargetEncoder]\n",
    "\n",
    "winning_classifier = [KNeighborsClassifier(n_neighbors=5, n_jobs=-1)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "for classifier in winning_classifier:\n",
    "    binary_features = binary_feats_list\n",
    "    binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=-1))])\n",
    "\n",
    "    numeric_features = num_feats_list\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler',StandardScaler())])\n",
    "\n",
    "\n",
    "    for encoder in winning_encoder:\n",
    "        print(\"Encoder: \")\n",
    "        print(encoder)\n",
    "        categorical_features = string_feats_list\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('woe', encoder())])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                ('binary', binary_transformer, binary_features)])\n",
    "\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('clf', classifier)])\n",
    "        print(\"Classifier: \")\n",
    "        print(classifier)\n",
    "        print(\"<=================================================================================================>\")\n",
    "        grid_classifier = GridSearchCV(pipe, search_space, cv=3, verbose=0).fit(X,y.values.ravel())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_best: \n",
      "5\n"
     ]
    }
   ],
   "source": [
    "k_best = grid_classifier.best_estimator_.get_params()[\"clf__n_neighbors\"]\n",
    "print(\"k_best: \")\n",
    "print(k_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

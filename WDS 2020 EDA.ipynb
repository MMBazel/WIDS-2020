{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(style='white')\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# Need to go find this one: from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_types(df, num_list, binary_list, string_list):\n",
    "    # Ensure datatypes are correct\n",
    "    for i in df.columns:\n",
    "        if i in num_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in binary_list:\n",
    "            df[i] = df[i].astype('Int64')\n",
    "        elif i in string_list:\n",
    "            df[i] = df[i].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_types_V2(df, num_list, binary_list, string_list):\n",
    "    for i in df.columns:\n",
    "        if i in num_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in binary_list:\n",
    "            df[i] = df[i].astype('Float64')\n",
    "        elif i in string_list:\n",
    "            df[i] = df[i].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the necessary files\n",
    "\n",
    "data_dictionary = pd.read_csv('WiDS Datathon 2020 Dictionary.csv')\n",
    "data = pd.read_csv('training_v2.csv')\n",
    "predict_data = pd.read_csv('unlabeled.csv')\n",
    "sample_submission = pd.read_csv('samplesubmission.csv')\n",
    "submission_template = pd.read_csv('solution_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies for safe keeping\n",
    "data_copy = data.copy()\n",
    "predict_data_copy = predict_data.copy()\n",
    "data_dictionary_copy = data_dictionary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns for easier selection in Data Dictionary\n",
    "data_dictionary.columns = ['Category','VariableName','UnitofMeasure','DataType','Descrption','Example']\n",
    "\n",
    "\n",
    "# Preoperly relabel the meta data; Meta data list will be used to pull features and process by type\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'encounter_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'hospital_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'patient_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'hospital_death', 'DataType'] = 'Target'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'bmi', 'DataType'] = 'numeric'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'icu_id', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'apache_2_diagnosis', 'DataType'] = 'string'\n",
    "data_dictionary.loc[data_dictionary.VariableName == 'apache_3j_diagnosis', 'DataType'] = 'string'\n",
    "\n",
    "\n",
    "# data_dictionary.head()\n",
    "# This checks if any DataTypes are null\n",
    "# print(data_dictionary[data_dictionary.DataType.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train & test data for cleaning & feature engineering\n",
    "\n",
    "#data = pd.concat([train_data, predict_data])\n",
    "#data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names by type\n",
    "\n",
    "num_feats_list = []\n",
    "binary_feats_list = []\n",
    "string_feats_list =[]\n",
    "\n",
    "variable_names = list(set(data_dictionary.VariableName))\n",
    "data_dictionary = data_dictionary.set_index('VariableName')\n",
    "#print(variable_names)\n",
    "\n",
    "for i in variable_names:\n",
    "    #print(data_dictionary.loc[i, 'DataType'])\n",
    "    if (i == 'VariableName') | (i == 'pred') | (i == 'icu_admit_type'):\n",
    "        pass\n",
    "    else :\n",
    "        if data_dictionary.loc[i, 'DataType'] == 'string':\n",
    "            # print(\"Is String:\" + i)\n",
    "            string_feats_list.append(i)    \n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'binary':\n",
    "            # print(\"Is Binary:\" + i)\n",
    "            binary_feats_list.append(i)\n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'integer':\n",
    "            #print(\"Is Numeric: \" + i)\n",
    "             num_feats_list.append(i)\n",
    "        elif data_dictionary.loc[i, 'DataType'] == 'numeric':\n",
    "            # print(\"Is Numeric: \" + i)\n",
    "             num_feats_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datatypes are correct\n",
    "dfs = [data, predict_data]\n",
    "for i in dfs:\n",
    "    i = ensure_data_types(i,num_feats_list, binary_feats_list, string_feats_list)\n",
    "\n",
    "\n",
    "#for i in data.columns:\n",
    "#    if i in num_feats_list:\n",
    "#        data[i] = data[i].astype('Float64')\n",
    "#    elif i in binary_feats_list:\n",
    "#        data[i] = data[i].astype('Int64')\n",
    "#    elif i in string_feats_list:\n",
    "#        data[i] = data[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train & predicts back in order to perform upsampling and feature selection\n",
    "not_missing = []\n",
    "missing_columns = []\n",
    "for i in data.columns:\n",
    "    if i in num_feats_list:\n",
    "        not_missing.append(i)\n",
    "    elif i in binary_feats_list:\n",
    "        not_missing.append(i)\n",
    "    elif i in string_feats_list:\n",
    "        not_missing.append(i)\n",
    "    else:\n",
    "        missing_columns.append(i)\n",
    "\n",
    "\n",
    "\n",
    "#train_eng = data[data.encounter_id.isin(train_data.encounter_id)]\n",
    "\n",
    "\n",
    "\n",
    "#predict_eng = data[data.encounter_id.isin(predict_data.encounter_id)]\n",
    "\n",
    "# Upsampling to address imbalanced data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [data, predict_data]\n",
    "for i in dfs:\n",
    "    i = ensure_data_types_V2(i,num_feats_list, binary_feats_list, string_feats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datatypes are correct\n",
    "#for i in train_eng.columns:\n",
    "#    if i in num_feats_list:\n",
    "#        train_eng[i] = train_eng[i].astype('Float64')\n",
    "#    elif i in binary_feats_list:\n",
    "#        train_eng[i] = train_eng[i].astype('Float64')\n",
    "#    elif i in string_feats_list:\n",
    "#        train_eng[i] = train_eng[i].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Colinear Columns\n",
    "\n",
    "Collinear features are features that are highly correlated with one another. In machine learning, these lead to decreased generalization performance on the test set due to high variance and less model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_death</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>elective_surgery</th>\n",
       "      <th>height</th>\n",
       "      <th>pre_icu_los_days</th>\n",
       "      <th>readmission_status</th>\n",
       "      <th>weight</th>\n",
       "      <th>albumin_apache</th>\n",
       "      <th>apache_post_operative</th>\n",
       "      <th>...</th>\n",
       "      <th>apache_4a_hospital_death_prob</th>\n",
       "      <th>apache_4a_icu_death_prob</th>\n",
       "      <th>aids</th>\n",
       "      <th>cirrhosis</th>\n",
       "      <th>diabetes_mellitus</th>\n",
       "      <th>hepatic_failure</th>\n",
       "      <th>immunosuppression</th>\n",
       "      <th>leukemia</th>\n",
       "      <th>lymphoma</th>\n",
       "      <th>solid_tumor_with_metastasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>hospital_death</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111017</td>\n",
       "      <td>0.031247</td>\n",
       "      <td>0.093574</td>\n",
       "      <td>0.019526</td>\n",
       "      <td>0.063316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038362</td>\n",
       "      <td>0.193809</td>\n",
       "      <td>0.083674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311043</td>\n",
       "      <td>0.283913</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.039453</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>0.038864</td>\n",
       "      <td>0.043973</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.051105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>0.111017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087077</td>\n",
       "      <td>0.067320</td>\n",
       "      <td>0.109937</td>\n",
       "      <td>0.049872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127252</td>\n",
       "      <td>0.116633</td>\n",
       "      <td>0.059246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143167</td>\n",
       "      <td>0.076275</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.077908</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>0.025007</td>\n",
       "      <td>0.030310</td>\n",
       "      <td>0.023335</td>\n",
       "      <td>0.025924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bmi</td>\n",
       "      <td>0.031247</td>\n",
       "      <td>0.087077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015921</td>\n",
       "      <td>0.056316</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.877339</td>\n",
       "      <td>0.052009</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033546</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>0.020434</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.031144</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>0.010017</td>\n",
       "      <td>0.043380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>elective_surgery</td>\n",
       "      <td>0.093574</td>\n",
       "      <td>0.067320</td>\n",
       "      <td>0.015921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023620</td>\n",
       "      <td>0.133704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.908247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.061250</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>0.031512</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.014695</td>\n",
       "      <td>0.017587</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.015369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>height</td>\n",
       "      <td>0.019526</td>\n",
       "      <td>0.109937</td>\n",
       "      <td>0.056316</td>\n",
       "      <td>0.023620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.391967</td>\n",
       "      <td>0.061671</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.011588</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.010481</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>0.004921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  hospital_death       age       bmi  elective_surgery  \\\n",
       "hospital_death          1.000000  0.111017  0.031247          0.093574   \n",
       "age                     0.111017  1.000000  0.087077          0.067320   \n",
       "bmi                     0.031247  0.087077  1.000000          0.015921   \n",
       "elective_surgery        0.093574  0.067320  0.015921          1.000000   \n",
       "height                  0.019526  0.109937  0.056316          0.023620   \n",
       "\n",
       "                    height  pre_icu_los_days  readmission_status    weight  \\\n",
       "hospital_death    0.019526          0.063316                 NaN  0.038362   \n",
       "age               0.109937          0.049872                 NaN  0.127252   \n",
       "bmi               0.056316          0.001531                 NaN  0.877339   \n",
       "elective_surgery  0.023620          0.133704                 NaN  0.026900   \n",
       "height            1.000000          0.008075                 NaN  0.391967   \n",
       "\n",
       "                  albumin_apache  apache_post_operative  ...  \\\n",
       "hospital_death          0.193809               0.083674  ...   \n",
       "age                     0.116633               0.059246  ...   \n",
       "bmi                     0.052009               0.015420  ...   \n",
       "elective_surgery        0.024966               0.908247  ...   \n",
       "height                  0.061671               0.025276  ...   \n",
       "\n",
       "                  apache_4a_hospital_death_prob  apache_4a_icu_death_prob  \\\n",
       "hospital_death                         0.311043                  0.283913   \n",
       "age                                    0.143167                  0.076275   \n",
       "bmi                                    0.033546                  0.013796   \n",
       "elective_surgery                       0.098412                  0.061250   \n",
       "height                                 0.029071                  0.011588   \n",
       "\n",
       "                      aids  cirrhosis  diabetes_mellitus  hepatic_failure  \\\n",
       "hospital_death    0.004403   0.039453           0.015784         0.038864   \n",
       "age               0.029477   0.028065           0.077908         0.020061   \n",
       "bmi               0.020434   0.002377           0.172943         0.001855   \n",
       "elective_surgery  0.006229   0.031512           0.001645         0.034700   \n",
       "height            0.009290   0.012043           0.000980         0.010481   \n",
       "\n",
       "                  immunosuppression  leukemia  lymphoma  \\\n",
       "hospital_death             0.043973  0.029788  0.018722   \n",
       "age                        0.025007  0.030310  0.023335   \n",
       "bmi                        0.031144  0.013375  0.010017   \n",
       "elective_surgery           0.014695  0.017587  0.008215   \n",
       "height                     0.000530  0.001718  0.008370   \n",
       "\n",
       "                  solid_tumor_with_metastasis  \n",
       "hospital_death                       0.051105  \n",
       "age                                  0.025924  \n",
       "bmi                                  0.043380  \n",
       "elective_surgery                     0.015369  \n",
       "height                               0.004921  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = data.corr().abs()\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "print('There are %d columns to remove.' % (len(to_drop)))\n",
    "#Drop the columns with high correlations\n",
    "data = data.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove the collinear columns from the dataset\n",
    "\n",
    "for i in to_drop:\n",
    "    if i in num_feats_list:\n",
    "        num_feats_list.remove(i)\n",
    "    elif i in binary_feats_list:\n",
    "        binary_feats_list.remove(i)\n",
    "    elif i in string_feats_list:\n",
    "        string_feats_list.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split back to Train & Predict Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikiko.bazeley\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns = ['hospital_death'])\n",
    "\n",
    "\n",
    "y = data[['hospital_death']]\n",
    "\n",
    "y['hospital_death'] = y['hospital_death'].astype('Float64')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines (Data Cleaning, Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    #RandomForestClassifier(),\n",
    "    #AdaBoostClassifier(),\n",
    "    #GradientBoostingClassifier(),\n",
    "    #XGBClassifier(),\n",
    "    lgb.LGBMClassifier()\n",
    "    ]\n",
    "\n",
    "\n",
    "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder, \n",
    "#               ce.basen.BaseNEncoder,\n",
    "#               ce.binary.BinaryEncoder,\n",
    "#                ce.cat_boost.CatBoostEncoder,\n",
    "#                ce.hashing.HashingEncoder,\n",
    "                ce.helmert.HelmertEncoder,\n",
    "#                ce.james_stein.JamesSteinEncoder,\n",
    "                ce.one_hot.OneHotEncoder,\n",
    "#               ce.leave_one_out.LeaveOneOutEncoder,\n",
    "#                ce.m_estimate.MEstimateEncoder,\n",
    "#                ce.ordinal.OrdinalEncoder,\n",
    "                ce.polynomial.PolynomialEncoder,\n",
    "                ce.sum_coding.SumEncoder,\n",
    "                ce.target_encoder.TargetEncoder,\n",
    "                ce.woe.WOEEncoder\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# Numeric_transformation list\n",
    "\n",
    "# Ordinal transformation list (i.e. using label encoder)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    binary_features = binary_feats_list\n",
    "    binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=-1))])\n",
    "\n",
    "    numeric_features = num_feats_list\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "\n",
    "    for encoder in encoder_list:\n",
    "        print(\"Encoder: \")\n",
    "        print(encoder)\n",
    "        categorical_features = string_feats_list\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('woe', encoder())])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                ('binary', binary_transformer, binary_features)])\n",
    "\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('clf', classifier)])\n",
    "        pipe.fit(X_train, y_train.values.ravel())  \n",
    "        print (\"Classifier: \")\n",
    "        print(classifier)\n",
    "        print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "        print(cross_val_score(estimator=pipe, X=X_train, y=y_train.values.ravel(), cv = 3, scoring = 'roc_auc'))\n",
    "        print(\"<=================================================================================================>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: \n",
      "<class 'category_encoders.helmert.HelmertEncoder'>\n",
      "Classifier: \n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "model score: 0.930\n",
      "[0.89900409 0.89793564 0.89069815]\n",
      "<=================================================================================================>\n"
     ]
    }
   ],
   "source": [
    "winning_classifier = [lgb.LGBMClassifier()]\n",
    "\n",
    "\n",
    "winning_encoder = [ce.helmert.HelmertEncoder]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "\n",
    "for classifier in winning_classifier:\n",
    "    binary_features = binary_feats_list\n",
    "    binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=-1))])\n",
    "\n",
    "    numeric_features = num_feats_list\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "\n",
    "    for encoder in winning_encoder:\n",
    "        print(\"Encoder: \")\n",
    "        print(encoder)\n",
    "        categorical_features = string_feats_list\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('woe', encoder())])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                ('binary', binary_transformer, binary_features)])\n",
    "\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('clf', classifier)])\n",
    "        pipe.fit(X_train, y_train.values.ravel())  \n",
    "        print (\"Classifier: \")\n",
    "        print(classifier)\n",
    "        print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "        print(cross_val_score(estimator=pipe, X=X_train, y=y_train.values.ravel(), cv = 3, scoring = 'roc_auc'))\n",
    "        print(\"<=================================================================================================>\")\n",
    "        \n",
    "        \n",
    "        # Make predictions on the predict_data        \n",
    "        features = num_feats_list + binary_feats_list + string_feats_list\n",
    "        \n",
    "        X_predict = predict_data.drop(columns=['hospital_death'])\n",
    "        \n",
    "        predict_data['hospital_death'] = pipe.predict_proba(X_predict[features])[:,1]\n",
    "\n",
    "        # Prepare submissions\n",
    "        submission = predict_data[['encounter_id','hospital_death']]\n",
    "\n",
    "        # Save submission file as .csv\n",
    "        submission.to_csv('MMBAZEL_WIDS2020_solution.csv')\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict & Prepare File For Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test data\n",
    "X_predict = predict_eng.drop(columns=['hospital_death'])\n",
    "predict_eng['hospital_deaths'] = win_clf.predict(X_predict[features])\n",
    "\n",
    "# Prepare submissions\n",
    "submission = predict_eng[['encounter_id','hospital_death']]\n",
    "\n",
    "# Save submission file as .csv\n",
    "submission.to_csv('solution_template.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
